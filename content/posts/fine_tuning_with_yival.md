---
title: "Fine-tuning LLMs and Visualize results with Yival Framework"
date: 2023-09-28T11:10:36+08:00
draft: false
language: en
featured_image: ../assets/images/featured/fine-tuning-with-yival-featured.jpg
summary: Fine-tuning Language Models (LLMs) using the Yival and effectively visualizing the results to gain deeper insights and facilitate a comprehensive understanding of the model's performance and capabilities. 
description: This is the description part.
author: YiVal
authorimage: ../assets/images/global/yival_author_img.jpeg
categories: Blog
tags: ['retrieval', 'faiss', 'prompt management']
---

# Fine-tuning LLMs and Visualize results with Yival Framework

## ****Fine-tuning LLMs****

Fine-tuning is when a pre-trained model is customized to assimilate new knowledge or specialize in a particular task, by learning with new data. In the context of LLMs, fine-tuning is typically used to transforming a general-purpose base model (e.g. GPT3) into a specialized model for a particular use case, or improve the base model for a specific task. 

The key advantage is this approach it that models can achieve better performance with limited data. Fine-tuning LLMs involves adjusting the weights and parameters of the pre-trained model using a smaller dataset specific to the desired task. 

## How YiVal helps in Fine-tuning

YiVal is a versatile but unified platform that streamlines the development of GenAI applications by providing flexible evaluation and fine-tuning capabilities for various building blocks, including model metadata, parameters, prompts, and retrieval configurations. 

Within this documentation, we aim to conduct a comprehensive evaluation of English-to-Chinese translation proficiency between llama2 and gpt-3.5-turbo. To establish a benchmark, we will employ the bertscore evaluator provided by yival. Given llama2's inherent limitations in Chinese translation as a base model, our strategy involves fine-tuning llama2 using the dataset generated by YiVal, allowing for a comparative analysis of its enhanced capabilities. 

****Steps****

1. Generate training data with YiVal data generator, powered by GPT-4
2. Compare Engilish-to-Chinese translation ability of llama2 and GPT-3.5
3. Fine-tuning llama2 with GPT-4 as a teacher
4. Re-Evaluate and visualized fine-tuning results

### 1. Generate Quiz Data

We use yival `openai_prompt_data_generator` to generate <English, Chinese> pair that generated and translated by GPT-4. 

```yaml
custom_function: demo.translate_quiz.translate_quiz
description: Generated experiment config
dataset:
  data_generators:
    openai_prompt_data_generator:
      chunk_size: 100000
      diversify: true
      # model_name specify the llm model , e.g. a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
      model_name: gpt-4
      prompt:
          "Please provide a concrete and realistic test case as a dictionary for function invocation using the ** operator.
          Only include parameters, excluding description and name.
          Ensure it's succinct and well-structured.
          **Only provide the dictionary.**"
      input_function:
        description:
          The current function is to evaluate the English to Chinese translation ability of the large language model. You will play the role of a teacher, so please provide a coherent English sentence (teacher_quiz), and give the corresponding Chinese translation (teachaer_answer).
        name: translation_english_to_chinese
        parameters:
          teacher_quiz: str
          teacher_answer: str
      expected_param_name: teacher_answer
      number_of_examples: 2
      output_path: english2chinese1.pkl
      call_option:
        temperature: 1.6
        presence_penalty: 2
  source_type: machine_generated
```

### 2. Compare models before Fine-tuning

Before fine-tuning, let take a look on how initial llama2 performs for this translation job. Using YiValâ€™s evaluators, we can easily compare it with GPT-3.5 regarding BertScore, token usage, and latency, or any other custom metrics. 

```yaml
variations:
  - name : model_name
    variations:
      - instantiated_value: gpt-3.5-turbo
        value: gpt-3.5-turbo
        value_type: str
        variation_id: null

      - instantiated_value: replicate/a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
        value: a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
        value_type: str
        variation_id: null
```

![**Experiment Result between llama2 and gpt-3.5-turbo**](https://github.com/KyleChen400/YiVal/assets/42785676/d5002670-eabc-4560-80a6-f1e68529aeae)
[Experiment Result between llama2 and gpt-3.5-turbo]

### 3. **Fine-tuning llama2 and Re-Evaluate**

We provide a real easy way for fine-tuning llama2. The data for fine-tuning can either originate from datasets that meet evaluator conditions or from data with expected values generated by GPT-4. For additional details, please refer to our **[replicate_finetune](https://github.com/YiVal/YiVal/blob/master/src/yival/dataset/replicate_finetune_utils.py)** repository. Upon completion of the fine-tuning process, you will obtain a designated model_name, allowing you to invoke the fine-tuned model using this identifier via the replicate function. 

```yaml
!poetry run python /content/YiVal/src/yival/dataset/replicate_finetune_utils.py 
```

**Fine-tuning Result**

For demo use case, we only use 400 examples, and fine-tuning with 10 epochs. Even with limited data, we saw significant performance improvement on llama2 regarding Englisg-to-Chinese translation. 

- bertscore-p 0.419 -> 0.445 (**+6.2%**)
- bertscore-r 0.592 -> 0.611   (**+3.2%**)
- bertscore-f 0.489 -> 0.514  (**+5.1%**)

![**Model performance comparison**](https://github.com/KyleChen400/YiVal/assets/42785676/9a633fbb-17c7-4e2b-ae41-f1e45319e0c1)
[Model performance comparison: sample fine-tuning]

**Fine-tuning results locally**

We also conduct a more complete fine-tuning locally with [EMNLP 2020 translation dataset](https://statmt.org/wmt20/translation-task.html). For the same 400 test data sample, we achieved much better performance.

- bertscore-p 0.516 -> 0.835 (**+61.8%**)
- bertscore-r 0.653 -> 0.828 (**+27.0%**)
- bertscore-f 0.575 -> 0.831 (**+44.5%**)

Also, we see after fine-tuning, our new llama2 model can achieve comparable performance as gpt-3.5.

![**Model performance comparison**](https://github.com/KyleChen400/YiVal/assets/42785676/a62782d6-1523-408c-a5ce-f7069c720f68)
[Model performance comparison: full fine-tuning]

## Conclusion

The fine-tuning process allows for the enhancement of llama2's English-to-Chinese translation abilities. By utilizing the YiVal platform and its fine-tuning capabilities, llama2 can be trained with a smaller dataset generated by GPT-4. The evaluation results show a significant improvement in performance, with an huge increase on bertscore. This demonstrates the effectiveness of fine-tuning LLMs for specific tasks, even with limited data.